{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <h1><strong>Evaluate a Resnet Model </strong></h1>\n",
    "    <h1><strong>Azure posgresql Database & Azure Blob Storage</strong></h1>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of the notebook:\n",
    "#### The purpose of the notebook is to load a Resnet model previously trained and to evaluate its performance  on a test dataset\n",
    "#### We assumed that the test dataset is generated according to the same approach than the one used to train the model (see train_resnet_model_local.ipynb or train_resnet_model_local.ipynb)\n",
    "#### Albeit images used for testing should not have been used during training for data leakage consideration (ie. no date overlap)\n",
    "#### Evaluation metrics displayed here are the ones that have been set while compiling the model before training (Loss, Accuracy, Precision and Recall)\n",
    "#### The notebook is meant to be used if the storage solution is set for <strong>Azure posgresql Database & Azure Blob Storage</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "### 1- Import of Packages and Dependencies\n",
    "### 2- Import Environment Variables\n",
    "### 3- Finding MLFlow experiments and runs \n",
    "### 4- Generate the test dataset\n",
    "### 5- Evaluate the model on the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Import of Packages and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from utils.build_dataset import *\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Import Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access environment variables using os.getenv() method\n",
    "# We need api_key and pai_url to connect to the API and get the data\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "api_url = os.getenv(\"API_URL\")\n",
    "\n",
    "# We need the follow variables to connect to the Azure Blob Storage\n",
    "container_name = os.getenv(\"AZURE_STORAGE_CONTAINER_NAME\")\n",
    "storage_account_name = os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "\n",
    "# We need the follow variables to connect to the Azure Posgresql Database\n",
    "pghost = os.getenv(\"PGHOST\")\n",
    "pguser = os.getenv(\"PGUSER\")\n",
    "pgport = os.getenv(\"PGPORT\")\n",
    "pgdatabase = os.getenv(\"PGDATABASE\")\n",
    "pgpassword = os.getenv(\"PGPASSWORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Finding MLFlow experiments and runs  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We first need to define the tracking URI for MLflow so that it can log the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first need to define the tracking URI for MLflow so that it can log the results\n",
    "tracking_uri=f\"postgresql://{pguser}:{pgpassword}@{pghost}:{pgport}/{pgdatabase}\"\n",
    "mlflow.set_tracking_uri(tracking_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can list our experiments and select the one we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all experiments\n",
    "experiments = mlflow.search_experiments()\n",
    "\n",
    "# Print the experiment details\n",
    "for experiment in experiments:\n",
    "    print(f\"Experiment ID: {experiment.experiment_id}\")\n",
    "    print(f\"Name: {experiment.name}\")\n",
    "    print(f\"Artifact Location: {experiment.artifact_location}\")\n",
    "    print(f\"Lifecycle Stage: {experiment.lifecycle_stage}\")\n",
    "    print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we choose the ID of the experiment we want to log the results to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we set the ID of the experiment we want to use\n",
    "experiment_id = \"7\"\n",
    "# We search for the runs in the experiment\n",
    "runs = mlflow.search_runs(experiment_ids=experiment_id)\n",
    "# We view the runs in the experiment (a pd.DataFrame)\n",
    "runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = mlflow.search_runs(experiment_ids=experiment_id)\n",
    "\n",
    "# Print the experiment details\n",
    "for index in runs.index:\n",
    "    print(f\"run ID: {runs[['run_id']].iloc[index]}\")\n",
    "    print(f\"Mlflow name: {runs[['tags.mlflow.runName']].iloc[index]}\")\n",
    "    print(f\"Artifact URI: {runs[['artifact_uri']].iloc[index]}\")\n",
    "    print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use here the first run of the experiment as an example (e.g. at the index 0).\n",
    "#### We need the run_id to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = mlflow.search_runs(experiment_ids=experiment_id).iloc[0].run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The run ID is : {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can load the model from the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the MLflow run\n",
    "model_uri = f\"runs:/{run_id}/model\"  # replace with the actual run ID\n",
    "\n",
    "# Load the Keras model\n",
    "loaded_model = mlflow.keras.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Generate the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We first need to get the name of the Resnet model from which our model was fine-tuned to propress the test dataset\n",
    "#### We can get this information from the MLflow run metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from MLFlow we call the parameters of the model and get the model_name (to be passed in the propressing function)\n",
    "params = mlflow.get_run(run_id).data.params\n",
    "model_name = params['model_name']\n",
    "print(f\"Model name: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We set the parameters to built the test dataset like we did for the training dataset\n",
    "#### We use a slighty different function to create the test dataset because we don't need to split the data\n",
    "#### Otherwise the pipeline is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"media\"\n",
    "labels = [\"vine\", \"grass\", \"ground\"]\n",
    "start_date = \"2021-03-01\"\n",
    "end_date = \"2021-05-01\"\n",
    "\n",
    "# We make the train and validation datasets\n",
    "image_urls = get_image_urls_with_multiple_labels(labels, start_date, end_date, api_key, api_url)\n",
    "# Download images and create a sample map\n",
    "df_sample_map = create_sample_map(image_urls)\n",
    "df_sample_map = download_images(df_sample_map, image_dir)\n",
    "\n",
    "test_dataset = create_test_dataset(df_sample_map,\n",
    "                              image_dir = 'media',\n",
    "                              model_name = model_name,\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-  Evaluate the model on the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can evaluate the model on the test dataset\n",
    "#### The metrics displayed here are the ones that have been set while compiling the model before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = loaded_model.evaluate(test_dataset)\n",
    "\n",
    "# We print the results\n",
    "print(f\"Test Loss: {results[0]}\")\n",
    "print(f\"Test Accuracy: {results[1]}\")\n",
    "print(f\"Test Precision: {results[2]}\")\n",
    "print(f\"Test Recall: {results[3]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resnet_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
